---
title: "Lab 2: What Makes a Product Successful?"
author: 'w203: Statistics for Data Science'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Imagine that you are part of a team of product data scientists at Acme, Inc. Your manager, Dr. Coy Ote, has given you the freedom to choose your own product to investigate, and evaluate a way to make it more successful.

**Your task is to select and develop a research question, find appropriate data, then write a proposal for the research study you will perform.** 

# Research Question

Your research question must be specific, it should clearly state an $X$ and a $Y$, defined at a conceptual level.  Your $X$ should be a design property or characteristic of a product that could be modified in the production process, and your $Y$ should be a metric of success.

In selecting your research question, you will have to use the skills you developed in RDADA to work on a question that can be addressed using a regression analysis. It is not appropriate to ask "What product features increase success?" or "How does product design affect sales?". These types of questions are not amenable to a modeling based approach and your study would likely become a fishing expedition. Instead, your team will have to use your background knowledge to identify a relationship you want to measure between a specific design feature and a specific metric of success.

If your data set is large enough, you can begin your process by splitting the data into an exploration set and a confirmation set.  As a rough guideline, you might put 30\% of your data into the exploration set, but make sure that both sets have a minimum of 100-200 rows of data.  Use the exploration set to build your intuition, explore the data, and build your model specifications.  In the ideal case, all *modeling decisions* that you make are based on the exploration set. The confirmation set should be used only once the code to generate your regression table and other results is set. All numbers in your report, as well as your discussion and conclusions, should be based on your confirmation set.

Because your manager is interested in *changes* to a product, they are fundamentally asking you to perform an explanatory study.  As we have noted in the class, given observational data, an OLS regression is usually not a credible way to measure a causal effect.  We have purposefully selected a domain in which the one-equation structural model is at least partially defensible.  The most prominent causal pathways will go in one direction, from product design characteristics to success.  While not a perfect reflection of reality, we expect your model to be plausible enough to make your results interesting.  At the same time, you will need to analyze potential violations of the one-equation structural model and what effect any violations may have on your results.

# Data

For this lab, you and your team will be responsible for gathering the data that you use. The data should be publicly available, and should be relevant to your research question. To increase the diversity of products investigated, we are asking students to avoid working on data that is sourced from Yelp and Airbnb. Also, please do not use the Ames housing data analyzed in the sample answer. There are many public data resources available, for example: 

- [New York Times](https://open.nytimes.com/data/home)
- [Tidy Tuesday](https://github.com/rfordatascience/tidytuesday)
- [ICPSR](https://www.icpsr.umich.edu/web/pages/) for social and political data 
- [Data.world](https://data.world/datasets/products)
- [Dataverse](https://dataverse.harvard.edu) for published research data
- [Data is Plural](https://www.data-is-plural.com)
- [UC Irvine Machine Learning Data Repository](https://archive.ics.uci.edu/ml/index.php)
- [Google Dataset Search](https://datasetsearch.research.google.com)
- [Amazon Open Data Registry](https://registry.opendata.aws)
- [Azure Open Data Registry](https://docs.microsoft.com/en-us/azure/open-datasets/dataset-catalog)

**Requirements for your data:**

- Data should be cross-sectional (i.e. not have multiple measurements for a single unit of observation). You may, for example, take a single cross section from a larger panel, or combine measurements from different time periods into a single number.
- We recommend a minimum of 100 or 200 observations. A team can choose an interesting dataset that is smaller than this, however, this will then require the team to assess and satisfy the more stringent CLM assumptions. 
- The outcome (or outcomes) that you use should be plausibly metric (i.e. number of sales of a product; number of views of a video). For this lab however, to make it easier to find data, teams may use an ordinal outcome variable if necessary. If using an ordinal outcome such as a 1-7 Likert scale, the team should clearly highlight this limitation in their report.
- For any omitted variable that would call your results into question, the data should include the best possible variable that operationalizes this concept. At a minimum, the data should have a variable that serves as an imperfect measure - or *proxy* - for the omitted variable.

You may draw different variables from different data sources.  You may use data sources not on the above list. You must document any data source you use in your report.

## Example of a Research Question 

Suppose that your team is interested in learning how the length of lanyard attached to a catapult affects performance. (A classic question from [Roadrunner cartoons](https://youtu.be/r9ENQ5j2zG4?t=15 ).)

You work to develop a primary outcome: proportion of boulders that land on their target.

On Acmeâ€™s servers, you find data on lanyard length, maximum-rated weight for the catapult and sales region. However, when you are reasoning about the product, you also note that length of the catapult arm and size of the catapult wheels are also likely to affect performance and are correlated with lanyard length. Because any model that does not include these confounding variables would yield estimates that conflate the importance of wheels and arms with the lanyard, you determine that the off-the-shelf data is not complete and that you need to encode the data yourself.

In the modeling phase of your project, your team proposes to build three models. One model estimates the relationship between targeting accuracy and lanyard length by itself.  A second model is similar, but adds a set of covariates including length of catapult arm and size of catapult wheels.  Finally, a third model includes an interaction term between lanyard length and customer type (first time or repeat), allowing you to investigate whether the effect of lanyard length is heterogeneous depending on the person operating the catapult.

\newpage
# Deliverable 1: A Research Proposal
## Due week 12, one submission per team

After a week of work, the project team will submit a research proposal. The maximum length is one page. This is so that your instructor can provide feedback to all teams quickly.

Please answer these three questions in your proposal:

1. What is the research question? Specifically, what is the $X$ concept and what is the $Y$ concept?
  1. Who is the actor who can change your $X$ concept? 
  2. Who is the audience who would care about changes in the $Y$ concept? 
2. What is the data source? What variables will you use to operationalize $X$ and $Y$?
3. What is the unit of observation? That is, does each row of the data represent a person, a review, a hotel stay, or something else?

The research proposal is intended to provide a structure for the team to have an early conversation with their instructor. It will be graded credit/no credit for completeness (i.e. a reasonable effort by the team will receive full marks), and the feedback that you will receive will be brief. Your project will be given either a *green*, *yellow* or *red* light. 

Projects that have been given a green-light can proceed with the question as formed, using the data as proposed. There will, quite naturally, be a lot of maturation that occurs over the course of the project, but a project that has received a green light can, and should, continue with their work. 

Projects that have been given a yellow-light can proceed with caution, and changes. Something about the proposal has raised a cause for concern in the eyes of the instructor. The question that the team has proposed might be too vague for a model-based answer, or you may have proposed a classic "fishing expedition" question (i.e. "Which of these features is the most important) which is not amenable to statistical analysis, or you might have proposed a data set that is known to be problematic. This evaluation will be accompanied by feedback from the instructor about what the team should do in order to move this project onto a path for success. 

Projects that have been given a red-light should be seriously considered, and likely re-considered by the project team. These are projects that, in the eyes of the instructor, seem likely to cause insurmountable problems for the team later on. Of course, the team may see a path to success with this type of project, but that path has not been made clear to the instructor when they read the team's proposal. 

\newpage
# Deliverable 2: Within-Team Review
## Due week 14, one submission per person

Being an effective, supportive team member is a crucial part of data science work. Your performance in this lab includes the role you play in supporting your teammates. This includes being responsive, creating an environment in which all members feel included, and above all treating each other with respect. In line with this perspective, we will ask each team member to write two paragraphs to their instructor about the progress they have made individually, and the team has made as a whole toward completing their report. 

This self-assessment should: 

- Reflect on the strengths and weakness, and the team's process in the project.
   - Where has your collaboration has worked well? How will you work to ensure that these successful practices continue to be employed? 
   - If there are places where collaboration has been challenging, what could the team have done jointly to improve? 
- If there are any individual performances that deserve special recognition, please let your instructor know in this evaluation.
- If there are any individual performances that require special attention, please also let your instructor know in this evaluation. 

Instructors will treat these reviews as anonymous. As well, because this is a submission as a part of your educational record, you have a federally protected guarantee that we will not share what you write with anyone outside of instructors with a legitimate educational purpose. 

**This reflection is due at the conclusion of your lab-work, in Gradescope, and requires one submission per person.**

\newpage 
# Deliverable 3: Final Presentation 
## Presented week 14, one presentation per team

The final presentation should show the team's work. While it is not a requirement that each person actually present (although you may, if you like) your instructor expects that each team-member will contribute to writing the presentation. 

The audience of your presentation is your class -- who you can imagine are interested, statistically informed attendees who have the goal of making your work better by providing direct feedback. You do not need to, for example describe how a t-test works, but if you choose to use such a test, you must use it appropriately.

Time in presentations, especially presentations that have results within them, *always* moves more quickly than you anticipate. Please, prepare to give your presentation within the allotted time. 

## Presentation Guidelines 

- **Plan for a 10 minute presentation.** Please note that this is an *incredibly* limited amount of time to present. A good rule of thumb is to use a maximum of 5 slides.
- If time is available, an additional 5 minutes will be devoted to questions.
- Begin by setting up your research question. It is quite alright to have a slide that bluntly states: "**Research Question**: [The question...]
- You should ground the audience in an understanding of the concepts that are important to your question, and the data that you will use to measure these concepts. Explain enough about your key variables that your audience can reason about your models and results. (2-3 minutes max) 
- Do not present R code, discuss data wrangling, or normality - details like this are best left to the full analysis. It is tempting to want to share these process based stories with your peers, but save that time for after the presentation. 
- It is a good practice to be very clear about the variables that you are including in your final model. It is also good practice to show the results of the regression either in a table or an instructive figure, which you talk about and allow your audience to read and reason about. If you show a regression table, you need to provide your audience with enough time to digest it (minimum 2 minutes). For any table (or plot) that you show, you should minimally interpret the variables (or axes) and the key point that you are making with that piece of evidence. 

Finally, a few more general thoughts: 

- Practice your talk with a timer!
- If you divide your talk with your teammates, practice your section with a timer to make sure you do not talk into your teammates' time. We would hate to cut your group off before a teammate has a chance to talk.

\newpage
# Deliverable 4: Final Report
## Due week 14, one submission per team

Your final report should document your analysis, communicating your findings in a way that is technically precise, clear, and persuasive.

**The maximum length is 4 pages using standard pdf_document output in RStudio, and including all tables, appendices, and references. This limit is strict.**

The exact format of your report is flexible (form follows function), but it should include the following elements.

## 1. An Introduction

Your introduction should present a research question and motivate its importance. It should draw the reader's attention to specific X and Y concepts in a way that makes the reader care about them. After reading the introduction, the reader should be prepared to understand why the models are constructed the way that they are. It is not enough to simply say, "We are looking for product features that enhance product success."  Your introduction must do work for you, focusing the reader on a specific measurement goal, making them care about it, and propelling the narrative forward. This is also a good time to put your work into context, discuss cross-cutting issues, and assess the overall appropriateness of the data.

## 2. A Brief Description of the Data

You should assume that your reader is not familiar with the data you are using. Provide basic information such as the organization that collected the data, whether it is experimental or observational, and how units of observation were selected,.

## 3. A Discussion of How Key Concepts are Operationalized

You should explain which variables are used to represent your X and your Y, and how well they match these concepts. Identify key gaps between the conceptual and operational definitions. If there are alternative variables that you considered, highlight them and explain how you made your decision.

## 4. An Explanation of Key Modeling Decisions

1. How many observations were removed from the data, and for what reasons?
2. What transformations did you apply to your variables and why? Are they supported by scatterplots, statistical tests, or existing theory?
3. Are there covariates that were intentionally left out of your models and why? For example, did they reduce your precision too much, or are they outcome variables?

## 5. A Table or Visualization

You will be graded on your visual design. In particular:

1. Plots should be easy to navigate, with useful titles and axis labels.
2. Do not include raw R output. All output, including variable names, should be formatted to make it easy for an English speaker to read.
3. Plots should have a high information-to-ink ratio. If you are only communicating 2-4 numbers, a table is generally more effective than a plot.
4. Any plot or table you include must be commented on in your narrative. In other words, no output dumps!

## 6. A Well-Formatted Regression Table.

It is important to remember that you are not trying to create one perfect model. You will create several specifications, giving the reader a sense of how robust (or sensitive) your results are to modeling choices, and to show that you're not just cherry-picking the specification that leads to the largest effects.

You should display all of your model specifications in a regression table, using a package like [`stargazer`](https://cran.r-project.org/web/packages/stargazer/vignettes/stargazer.pdf) to format your output. It should be easy for the reader to find the coefficients that represent key effects near the top of the regression table, and scan horizontally to see how they change from specification to specification. Make sure that you display the most appropriate standard errors in your table.

As you select your model specification, your goal is to encircle the space of reasonable modeling choices, and to give an overall understanding of how these choices impact results. You should strive to make your models different from each other. However, each individual model must be defensible.

At a minimum, you need to estimate at least three model specifications.

The first model you create should include *only the key variables* you want to measure. These variables might be transformed, as determined by your EDA, but the model should include the absolute minimum number of covariates (usually zero or one covariate that is so crucial it would be unreasonable to omit it).

The structure of the other models is more flexible. Most often, you will see researchers add a block of covariates from one model to the next. Each model should be defensible, and should continue to tell the story of how product features contribute to product success. This might mean including additional covariates to remove omitted variable bias; or, instead, it might mean estimating a model that operationalizes your X or Y in a different way (be sure the operationalization is substantially different). You may also create a model tailored to investigating a heterogeneous effect.

## 7. A Discussion of Results

In your text, comment on both *statistical significance and practical significance*. You may want to include statistical tests besides the standard t-tests for regression coefficients. Here, it is important that you make clear to your audience the practical significance of any model results. How should the product change as a result of what you have discovered? Are there limits to how much change you are proposing? What are the most important results that you have discovered, and what are the least important? 

## 8. A Discussion of Limitations

### 8a. Statistical limitations of your model

Make sure to evaluate all of the large sample model assumptions (or the CLM if you have a small sample). However, you do not necessarily want to discuss every assumption in your report. Instead, highlight any assumption that might pose significant problems for your analysis. For any violations that you identify, describe the statistical consequences. If you are able to identify any strategies to mitigate the consequences, explain these strategies. 

Note that you may need to change your model specifications in response to violations of the large sample model.

### 8b. Structural limitations of your model

What are the most important *omitted variables* that you were not able to include? For each variable you name, *reason about the direction of bias* caused by omitting this variable and whether the omission of this variable calls into question the core results you are reporting. 

Is there a possibility of reverse causality? If so, *reason about the direction of bias* this causes.

Are there any outcome variables on the right hand side? If so, *reason about the direction of bias* this causes.

## 9. A Conclusion

Make sure that you end your report with a discussion that distills key insights from your work, addresses your research question, and leaves the reader with a sense of broader context or impact.